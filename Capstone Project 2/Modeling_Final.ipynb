{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing/training notebook the accuracy came out to be very unsatisfactory for the baseloine results. Overfitting is a big concern. I will be testing 2 additional models over and above Logistic Regression and Random Forest Classifier. **XGBoost** and **LGBM**, paired with 2 oversampling methods **SMOTE** and **ADYSYN**.\n",
    "\n",
    "Changes from previous notebook:\n",
    "> Splitting minority class evenly 20/20 and concatenating to 80/20 majority class\n",
    "\n",
    "> Conducting dummy encoding after train test split on X_train and X_test\n",
    "\n",
    "> Removing Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lending_club_modeling_pick.pkl', 'rb') as pickle_file:\n",
    "    df = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "#pd.reset_option('display.float_format')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>purpose</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_d</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>next_pymnt_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>tot_coll_amt</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>open_acc_6m</th>\n",
       "      <th>open_act_il</th>\n",
       "      <th>open_il_12m</th>\n",
       "      <th>open_il_24m</th>\n",
       "      <th>mths_since_rcnt_il</th>\n",
       "      <th>total_bal_il</th>\n",
       "      <th>il_util</th>\n",
       "      <th>open_rv_12m</th>\n",
       "      <th>open_rv_24m</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>all_util</th>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <th>inq_fi</th>\n",
       "      <th>total_cu_tl</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>acc_open_past_24mths</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>mo_sin_old_il_acct</th>\n",
       "      <th>mo_sin_old_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_rev_tl_op</th>\n",
       "      <th>mo_sin_rcnt_tl</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>mths_since_recent_bc</th>\n",
       "      <th>mths_since_recent_bc_dlq</th>\n",
       "      <th>mths_since_recent_inq</th>\n",
       "      <th>mths_since_recent_revol_delinq</th>\n",
       "      <th>num_accts_ever_120_pd</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>num_actv_rev_tl</th>\n",
       "      <th>num_bc_sats</th>\n",
       "      <th>num_bc_tl</th>\n",
       "      <th>num_il_tl</th>\n",
       "      <th>num_op_rev_tl</th>\n",
       "      <th>num_rev_accts</th>\n",
       "      <th>num_rev_tl_bal_gt_0</th>\n",
       "      <th>num_sats</th>\n",
       "      <th>num_tl_120dpd_2m</th>\n",
       "      <th>num_tl_30dpd</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>num_tl_op_past_12m</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>hardship_flag</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600.00000</td>\n",
       "      <td>3600.00000</td>\n",
       "      <td>3600.00000</td>\n",
       "      <td>36</td>\n",
       "      <td>13.99000</td>\n",
       "      <td>123.03000</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>55000.00000</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>16770.00000</td>\n",
       "      <td>n</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>PA</td>\n",
       "      <td>5.91000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>12265.00000</td>\n",
       "      <td>675.00000</td>\n",
       "      <td>679.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>72.31284</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2765.00000</td>\n",
       "      <td>29.70000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4421.72392</td>\n",
       "      <td>4421.72000</td>\n",
       "      <td>3600.00000</td>\n",
       "      <td>821.72000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17897.00000</td>\n",
       "      <td>122.67000</td>\n",
       "      <td>18017.00000</td>\n",
       "      <td>17956.00000</td>\n",
       "      <td>564.00000</td>\n",
       "      <td>560.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>722.00000</td>\n",
       "      <td>144904.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>21.00000</td>\n",
       "      <td>4981.00000</td>\n",
       "      <td>36.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>722.00000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>9300.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>20701.00000</td>\n",
       "      <td>1506.00000</td>\n",
       "      <td>37.20000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>148.00000</td>\n",
       "      <td>128.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>76.90000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>178050.00000</td>\n",
       "      <td>7746.00000</td>\n",
       "      <td>2400.00000</td>\n",
       "      <td>13734.00000</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24700.00000</td>\n",
       "      <td>24700.00000</td>\n",
       "      <td>24700.00000</td>\n",
       "      <td>36</td>\n",
       "      <td>11.99000</td>\n",
       "      <td>820.28000</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>65000.00000</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>16770.00000</td>\n",
       "      <td>n</td>\n",
       "      <td>small_business</td>\n",
       "      <td>SD</td>\n",
       "      <td>16.06000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>10926.00000</td>\n",
       "      <td>715.00000</td>\n",
       "      <td>719.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>72.31284</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>21470.00000</td>\n",
       "      <td>19.20000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25679.66000</td>\n",
       "      <td>25679.66000</td>\n",
       "      <td>24700.00000</td>\n",
       "      <td>979.66000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>16953.00000</td>\n",
       "      <td>926.35000</td>\n",
       "      <td>18017.00000</td>\n",
       "      <td>17956.00000</td>\n",
       "      <td>699.00000</td>\n",
       "      <td>695.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.16422</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>204396.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>18005.00000</td>\n",
       "      <td>73.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>6472.00000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>111800.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9733.00000</td>\n",
       "      <td>57830.00000</td>\n",
       "      <td>27.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>113.00000</td>\n",
       "      <td>192.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>27.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>97.40000</td>\n",
       "      <td>7.70000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>314017.00000</td>\n",
       "      <td>39475.00000</td>\n",
       "      <td>79300.00000</td>\n",
       "      <td>24667.00000</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>60</td>\n",
       "      <td>10.78000</td>\n",
       "      <td>432.66000</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>63000.00000</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>16770.00000</td>\n",
       "      <td>n</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>IL</td>\n",
       "      <td>10.78000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11170.00000</td>\n",
       "      <td>695.00000</td>\n",
       "      <td>699.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34.54092</td>\n",
       "      <td>72.31284</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7869.00000</td>\n",
       "      <td>56.20000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>22705.92429</td>\n",
       "      <td>22705.92000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>2705.92000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17318.00000</td>\n",
       "      <td>15813.30000</td>\n",
       "      <td>18017.00000</td>\n",
       "      <td>17956.00000</td>\n",
       "      <td>704.00000</td>\n",
       "      <td>700.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.16422</td>\n",
       "      <td>Joint App</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>189699.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>10827.00000</td>\n",
       "      <td>73.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2081.00000</td>\n",
       "      <td>65.00000</td>\n",
       "      <td>14000.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>31617.00000</td>\n",
       "      <td>2737.00000</td>\n",
       "      <td>55.90000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>125.00000</td>\n",
       "      <td>184.00000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>101.00000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>218418.00000</td>\n",
       "      <td>18696.00000</td>\n",
       "      <td>6200.00000</td>\n",
       "      <td>14877.00000</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35000.00000</td>\n",
       "      <td>35000.00000</td>\n",
       "      <td>35000.00000</td>\n",
       "      <td>60</td>\n",
       "      <td>14.85000</td>\n",
       "      <td>829.90000</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>110000.00000</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>16770.00000</td>\n",
       "      <td>n</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>NJ</td>\n",
       "      <td>17.06000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14123.00000</td>\n",
       "      <td>785.00000</td>\n",
       "      <td>789.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>34.54092</td>\n",
       "      <td>72.31284</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7802.00000</td>\n",
       "      <td>11.60000</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>w</td>\n",
       "      <td>15897.65000</td>\n",
       "      <td>15897.65000</td>\n",
       "      <td>31464.01000</td>\n",
       "      <td>31464.01000</td>\n",
       "      <td>19102.35000</td>\n",
       "      <td>12361.66000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17928.00000</td>\n",
       "      <td>829.90000</td>\n",
       "      <td>17987.00000</td>\n",
       "      <td>17956.00000</td>\n",
       "      <td>679.00000</td>\n",
       "      <td>675.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.16422</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>301500.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>12609.00000</td>\n",
       "      <td>70.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6987.00000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>67300.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>23192.00000</td>\n",
       "      <td>54962.00000</td>\n",
       "      <td>12.10000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>36.00000</td>\n",
       "      <td>87.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>381215.00000</td>\n",
       "      <td>52226.00000</td>\n",
       "      <td>62500.00000</td>\n",
       "      <td>18000.00000</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10400.00000</td>\n",
       "      <td>10400.00000</td>\n",
       "      <td>10400.00000</td>\n",
       "      <td>60</td>\n",
       "      <td>22.45000</td>\n",
       "      <td>289.91000</td>\n",
       "      <td>F</td>\n",
       "      <td>F1</td>\n",
       "      <td>3</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>104433.00000</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>16770.00000</td>\n",
       "      <td>n</td>\n",
       "      <td>major_purchase</td>\n",
       "      <td>PA</td>\n",
       "      <td>25.37000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>10378.00000</td>\n",
       "      <td>695.00000</td>\n",
       "      <td>699.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>72.31284</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>21929.00000</td>\n",
       "      <td>64.50000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>w</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11740.50000</td>\n",
       "      <td>11740.50000</td>\n",
       "      <td>10400.00000</td>\n",
       "      <td>1340.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>16983.00000</td>\n",
       "      <td>10128.96000</td>\n",
       "      <td>18017.00000</td>\n",
       "      <td>17591.00000</td>\n",
       "      <td>704.00000</td>\n",
       "      <td>700.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.16422</td>\n",
       "      <td>Individual</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>331730.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>73839.00000</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>9702.00000</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>34000.00000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>27644.00000</td>\n",
       "      <td>4567.00000</td>\n",
       "      <td>77.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>128.00000</td>\n",
       "      <td>210.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>96.60000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>439570.00000</td>\n",
       "      <td>95768.00000</td>\n",
       "      <td>20300.00000</td>\n",
       "      <td>88097.00000</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    loan_amnt  funded_amnt  funded_amnt_inv  term  int_rate  installment  \\\n",
       "0  3600.00000   3600.00000       3600.00000    36  13.99000    123.03000   \n",
       "1 24700.00000  24700.00000      24700.00000    36  11.99000    820.28000   \n",
       "2 20000.00000  20000.00000      20000.00000    60  10.78000    432.66000   \n",
       "3 35000.00000  35000.00000      35000.00000    60  14.85000    829.90000   \n",
       "4 10400.00000  10400.00000      10400.00000    60  22.45000    289.91000   \n",
       "\n",
       "  grade sub_grade  emp_length home_ownership   annual_inc verification_status  \\\n",
       "0     C        C4          10       MORTGAGE  55000.00000        Not Verified   \n",
       "1     C        C1          10       MORTGAGE  65000.00000        Not Verified   \n",
       "2     B        B4          10       MORTGAGE  63000.00000        Not Verified   \n",
       "3     C        C5          10       MORTGAGE 110000.00000     Source Verified   \n",
       "4     F        F1           3       MORTGAGE 104433.00000     Source Verified   \n",
       "\n",
       "      issue_d pymnt_plan             purpose addr_state      dti  delinq_2yrs  \\\n",
       "0 16770.00000          n  debt_consolidation         PA  5.91000      0.00000   \n",
       "1 16770.00000          n      small_business         SD 16.06000      1.00000   \n",
       "2 16770.00000          n    home_improvement         IL 10.78000      0.00000   \n",
       "3 16770.00000          n  debt_consolidation         NJ 17.06000      0.00000   \n",
       "4 16770.00000          n      major_purchase         PA 25.37000      1.00000   \n",
       "\n",
       "   earliest_cr_line  fico_range_low  fico_range_high  inq_last_6mths  \\\n",
       "0       12265.00000       675.00000        679.00000         1.00000   \n",
       "1       10926.00000       715.00000        719.00000         4.00000   \n",
       "2       11170.00000       695.00000        699.00000         0.00000   \n",
       "3       14123.00000       785.00000        789.00000         0.00000   \n",
       "4       10378.00000       695.00000        699.00000         3.00000   \n",
       "\n",
       "   mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  \\\n",
       "0                30.00000                72.31284   7.00000  0.00000   \n",
       "1                 6.00000                72.31284  22.00000  0.00000   \n",
       "2                34.54092                72.31284   6.00000  0.00000   \n",
       "3                34.54092                72.31284  13.00000  0.00000   \n",
       "4                12.00000                72.31284  12.00000  0.00000   \n",
       "\n",
       "    revol_bal  revol_util  total_acc initial_list_status   out_prncp  \\\n",
       "0  2765.00000    29.70000   13.00000                   w     0.00000   \n",
       "1 21470.00000    19.20000   38.00000                   w     0.00000   \n",
       "2  7869.00000    56.20000   18.00000                   w     0.00000   \n",
       "3  7802.00000    11.60000   17.00000                   w 15897.65000   \n",
       "4 21929.00000    64.50000   35.00000                   w     0.00000   \n",
       "\n",
       "   out_prncp_inv  total_pymnt  total_pymnt_inv  total_rec_prncp  \\\n",
       "0        0.00000   4421.72392       4421.72000       3600.00000   \n",
       "1        0.00000  25679.66000      25679.66000      24700.00000   \n",
       "2        0.00000  22705.92429      22705.92000      20000.00000   \n",
       "3    15897.65000  31464.01000      31464.01000      19102.35000   \n",
       "4        0.00000  11740.50000      11740.50000      10400.00000   \n",
       "\n",
       "   total_rec_int  total_rec_late_fee  recoveries  collection_recovery_fee  \\\n",
       "0      821.72000             0.00000     0.00000                  0.00000   \n",
       "1      979.66000             0.00000     0.00000                  0.00000   \n",
       "2     2705.92000             0.00000     0.00000                  0.00000   \n",
       "3    12361.66000             0.00000     0.00000                  0.00000   \n",
       "4     1340.50000             0.00000     0.00000                  0.00000   \n",
       "\n",
       "   last_pymnt_d  last_pymnt_amnt  next_pymnt_d  last_credit_pull_d  \\\n",
       "0   17897.00000        122.67000   18017.00000         17956.00000   \n",
       "1   16953.00000        926.35000   18017.00000         17956.00000   \n",
       "2   17318.00000      15813.30000   18017.00000         17956.00000   \n",
       "3   17928.00000        829.90000   17987.00000         17956.00000   \n",
       "4   16983.00000      10128.96000   18017.00000         17591.00000   \n",
       "\n",
       "   last_fico_range_high  last_fico_range_low  collections_12_mths_ex_med  \\\n",
       "0             564.00000            560.00000                     0.00000   \n",
       "1             699.00000            695.00000                     0.00000   \n",
       "2             704.00000            700.00000                     0.00000   \n",
       "3             679.00000            675.00000                     0.00000   \n",
       "4             704.00000            700.00000                     0.00000   \n",
       "\n",
       "   mths_since_last_major_derog application_type  acc_now_delinq  tot_coll_amt  \\\n",
       "0                     30.00000       Individual         0.00000     722.00000   \n",
       "1                     44.16422       Individual         0.00000       0.00000   \n",
       "2                     44.16422        Joint App         0.00000       0.00000   \n",
       "3                     44.16422       Individual         0.00000       0.00000   \n",
       "4                     44.16422       Individual         0.00000       0.00000   \n",
       "\n",
       "   tot_cur_bal  open_acc_6m  open_act_il  open_il_12m  open_il_24m  \\\n",
       "0 144904.00000      2.00000      2.00000      0.00000      1.00000   \n",
       "1 204396.00000      1.00000      1.00000      0.00000      1.00000   \n",
       "2 189699.00000      0.00000      1.00000      0.00000      4.00000   \n",
       "3 301500.00000      1.00000      1.00000      0.00000      1.00000   \n",
       "4 331730.00000      1.00000      3.00000      0.00000      3.00000   \n",
       "\n",
       "   mths_since_rcnt_il  total_bal_il  il_util  open_rv_12m  open_rv_24m  \\\n",
       "0            21.00000    4981.00000 36.00000      3.00000      3.00000   \n",
       "1            19.00000   18005.00000 73.00000      2.00000      3.00000   \n",
       "2            19.00000   10827.00000 73.00000      0.00000      2.00000   \n",
       "3            23.00000   12609.00000 70.00000      1.00000      1.00000   \n",
       "4            14.00000   73839.00000 84.00000      4.00000      7.00000   \n",
       "\n",
       "   max_bal_bc  all_util  total_rev_hi_lim  inq_fi  total_cu_tl  inq_last_12m  \\\n",
       "0   722.00000  34.00000        9300.00000 3.00000      1.00000       4.00000   \n",
       "1  6472.00000  29.00000      111800.00000 0.00000      0.00000       6.00000   \n",
       "2  2081.00000  65.00000       14000.00000 2.00000      5.00000       1.00000   \n",
       "3  6987.00000  45.00000       67300.00000 0.00000      1.00000       0.00000   \n",
       "4  9702.00000  78.00000       34000.00000 2.00000      1.00000       3.00000   \n",
       "\n",
       "   acc_open_past_24mths  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "0               4.00000  20701.00000      1506.00000 37.20000   \n",
       "1               4.00000   9733.00000     57830.00000 27.10000   \n",
       "2               6.00000  31617.00000      2737.00000 55.90000   \n",
       "3               2.00000  23192.00000     54962.00000 12.10000   \n",
       "4              10.00000  27644.00000      4567.00000 77.50000   \n",
       "\n",
       "   chargeoff_within_12_mths  delinq_amnt  mo_sin_old_il_acct  \\\n",
       "0                   0.00000      0.00000           148.00000   \n",
       "1                   0.00000      0.00000           113.00000   \n",
       "2                   0.00000      0.00000           125.00000   \n",
       "3                   0.00000      0.00000            36.00000   \n",
       "4                   0.00000      0.00000           128.00000   \n",
       "\n",
       "   mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  mo_sin_rcnt_tl  mort_acc  \\\n",
       "0             128.00000                3.00000         3.00000   1.00000   \n",
       "1             192.00000                2.00000         2.00000   4.00000   \n",
       "2             184.00000               14.00000        14.00000   5.00000   \n",
       "3              87.00000                2.00000         2.00000   1.00000   \n",
       "4             210.00000                4.00000         4.00000   6.00000   \n",
       "\n",
       "   mths_since_recent_bc  mths_since_recent_bc_dlq  mths_since_recent_inq  \\\n",
       "0               4.00000                  69.00000                4.00000   \n",
       "1               2.00000                  37.00000                0.00000   \n",
       "2             101.00000                  37.00000               10.00000   \n",
       "3               2.00000                  37.00000                0.00000   \n",
       "4               4.00000                  12.00000                1.00000   \n",
       "\n",
       "   mths_since_recent_revol_delinq  num_accts_ever_120_pd  num_actv_bc_tl  \\\n",
       "0                        69.00000                2.00000         2.00000   \n",
       "1                         6.00000                0.00000         5.00000   \n",
       "2                         0.00000                0.00000         2.00000   \n",
       "3                         0.00000                0.00000         4.00000   \n",
       "4                        12.00000                0.00000         4.00000   \n",
       "\n",
       "   num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  \\\n",
       "0          4.00000      2.00000    5.00000    3.00000        4.00000   \n",
       "1          5.00000     13.00000   17.00000    6.00000       20.00000   \n",
       "2          3.00000      2.00000    4.00000    6.00000        4.00000   \n",
       "3          5.00000      8.00000   10.00000    2.00000       10.00000   \n",
       "4          6.00000      5.00000    9.00000   10.00000        7.00000   \n",
       "\n",
       "   num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  \\\n",
       "0        9.00000              4.00000   7.00000           0.00000   \n",
       "1       27.00000              5.00000  22.00000           0.00000   \n",
       "2        7.00000              3.00000   6.00000           0.00000   \n",
       "3       13.00000              5.00000  13.00000           0.00000   \n",
       "4       19.00000              6.00000  12.00000           0.00000   \n",
       "\n",
       "   num_tl_30dpd  num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
       "0       0.00000             0.00000             3.00000        76.90000   \n",
       "1       0.00000             0.00000             2.00000        97.40000   \n",
       "2       0.00000             0.00000             0.00000       100.00000   \n",
       "3       0.00000             0.00000             1.00000       100.00000   \n",
       "4       0.00000             0.00000             4.00000        96.60000   \n",
       "\n",
       "   percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  \\\n",
       "0           0.00000               0.00000    0.00000     178050.00000   \n",
       "1           7.70000               0.00000    0.00000     314017.00000   \n",
       "2          50.00000               0.00000    0.00000     218418.00000   \n",
       "3           0.00000               0.00000    0.00000     381215.00000   \n",
       "4          60.00000               0.00000    0.00000     439570.00000   \n",
       "\n",
       "   total_bal_ex_mort  total_bc_limit  total_il_high_credit_limit  \\\n",
       "0         7746.00000      2400.00000                 13734.00000   \n",
       "1        39475.00000     79300.00000                 24667.00000   \n",
       "2        18696.00000      6200.00000                 14877.00000   \n",
       "3        52226.00000     62500.00000                 18000.00000   \n",
       "4        95768.00000     20300.00000                 88097.00000   \n",
       "\n",
       "  hardship_flag disbursement_method debt_settlement_flag  TARGET  \n",
       "0             N                Cash                    N       0  \n",
       "1             N                Cash                    N       0  \n",
       "2             N                Cash                    N       0  \n",
       "3             N                Cash                    N       0  \n",
       "4             N                Cash                    N       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257221, 107)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 107)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['TARGET'] == 1].iloc[20:, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_encoding(df):\n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    #Sampling 500k from target 0\n",
    "    df_t0_sample = df[df['TARGET'] == 0].sample(n = 500000, random_state = 1)\n",
    "    #df_t1 = df[df['TARGET'] == 1]\n",
    "    #df = pd.concat([df_t0_sample, df_t1], ignore_index = True)\n",
    "    \n",
    "    #split into x and y arrays\n",
    "    y = df_t0_sample['TARGET']\n",
    "    X = df_t0_sample.drop('TARGET', axis = 1)\n",
    "    \n",
    "    #Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 1, stratify = y)\n",
    "    \n",
    "    df_t1 = df[df['TARGET'] == 1].iloc[:20, :]\n",
    "    df_t2 = df[df['TARGET'] == 1].iloc[20:, :]\n",
    "    print(df_t1.head())\n",
    "    \n",
    "    y1 = df_t1['TARGET'] #ytrain\n",
    "    X1 = df_t1.drop('TARGET', axis = 1) #xtrain\n",
    "    y2 = df_t2['TARGET'] #ytest\n",
    "    X2 = df_t2.drop('TARGET', axis = 1) #xtest\n",
    "    \n",
    "    #Concat \n",
    "    X_train = pd.concat([X_train, X1], ignore_index = True)\n",
    "    X_test = pd.concat([X_test, X2], ignore_index = True)\n",
    "    y_train = y_train.append(y1, ignore_index = True)\n",
    "    y_test = y_test.append(y2, ignore_index = True)\n",
    "    \n",
    "    #Dummies\n",
    "    X_train = dummy_encoding(X_train)\n",
    "    X_test = dummy_encoding(X_test)\n",
    "    \n",
    "    #Scale X\n",
    "#     sc = StandardScaler()\n",
    "#     sc.fit(X_train)\n",
    "#     X_train = sc.transform(X_train)\n",
    "#     X_test = sc.transform(X_test)    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         loan_amnt  funded_amnt  funded_amnt_inv  term  int_rate  installment  \\\n",
      "33636  35000.00000  35000.00000      35000.00000    60  20.99000    946.68000   \n",
      "427430 30000.00000  30000.00000      30000.00000    36  17.47000   1076.62000   \n",
      "487674 25000.00000  25000.00000      25000.00000    60  21.45000    682.68000   \n",
      "492122 10000.00000  10000.00000      10000.00000    60  19.03000    259.58000   \n",
      "521688  3000.00000   3000.00000       3000.00000    36  19.03000    110.02000   \n",
      "\n",
      "       grade sub_grade  emp_length home_ownership   annual_inc  \\\n",
      "33636      E        E5          10            OWN 105000.00000   \n",
      "427430     D        D1          10            OWN  70000.00000   \n",
      "487674     D        D5           8           RENT  36000.00000   \n",
      "492122     D        D3           1           RENT  30000.00000   \n",
      "521688     D        D3           4           RENT  35000.00000   \n",
      "\n",
      "       verification_status     issue_d pymnt_plan             purpose  \\\n",
      "33636             Verified 16770.00000          n  debt_consolidation   \n",
      "427430            Verified 17591.00000          n  debt_consolidation   \n",
      "487674            Verified 17563.00000          n  debt_consolidation   \n",
      "492122     Source Verified 17532.00000          n  debt_consolidation   \n",
      "521688            Verified 17532.00000          n               other   \n",
      "\n",
      "       addr_state      dti  delinq_2yrs  earliest_cr_line  fico_range_low  \\\n",
      "33636          MD 25.36000      0.00000       10865.00000       675.00000   \n",
      "427430         CT 23.26000      0.00000       12478.00000       730.00000   \n",
      "487674         FL  2.20000      1.00000       11048.00000       700.00000   \n",
      "492122         NY 33.28000      0.00000       11719.00000       755.00000   \n",
      "521688         WA  0.86000      0.00000       14641.00000       660.00000   \n",
      "\n",
      "        fico_range_high  inq_last_6mths  mths_since_last_delinq  \\\n",
      "33636         679.00000         0.00000                34.54092   \n",
      "427430        734.00000         0.00000                40.00000   \n",
      "487674        704.00000         1.00000                19.00000   \n",
      "492122        759.00000         0.00000                34.54092   \n",
      "521688        664.00000         0.00000                64.00000   \n",
      "\n",
      "        mths_since_last_record  open_acc  pub_rec   revol_bal  revol_util  \\\n",
      "33636                 72.31284  20.00000  0.00000 19782.00000    98.90000   \n",
      "427430                72.31284   8.00000  0.00000 19359.00000    61.50000   \n",
      "487674                58.00000   3.00000  1.00000   923.00000    18.50000   \n",
      "492122                72.31284  14.00000  0.00000  7993.00000    12.40000   \n",
      "521688                72.31284  13.00000  0.00000   311.00000    31.10000   \n",
      "\n",
      "        total_acc initial_list_status   out_prncp  out_prncp_inv  total_pymnt  \\\n",
      "33636    60.00000                   w 21999.94000    21999.94000  30603.57000   \n",
      "427430   14.00000                   w 25320.47000    25320.47000   7507.22000   \n",
      "487674   12.00000                   w 22991.22000    22991.22000   6832.88000   \n",
      "492122   28.00000                   w  9032.08000     9032.08000   2325.65000   \n",
      "521688   18.00000                   f  2305.04000     2305.04000   1327.01000   \n",
      "\n",
      "        total_pymnt_inv  total_rec_prncp  total_rec_int  total_rec_late_fee  \\\n",
      "33636       30603.57000      13000.06000    17556.18000            47.33000   \n",
      "427430       7507.22000       4679.53000     2827.69000             0.00000   \n",
      "487674       6832.88000       2008.78000     4789.97000            34.13000   \n",
      "492122       2325.65000        967.92000     1357.73000             0.00000   \n",
      "521688       1327.01000        694.96000      587.05000            45.00000   \n",
      "\n",
      "        recoveries  collection_recovery_fee  last_pymnt_d  last_pymnt_amnt  \\\n",
      "33636      0.00000                  0.00000   17956.00000        946.68000   \n",
      "427430     0.00000                  0.00000   17836.00000       1076.62000   \n",
      "487674     0.00000                  0.00000   17956.00000       1401.23000   \n",
      "492122     0.00000                  0.00000   17836.00000        259.58000   \n",
      "521688     0.00000                  0.00000   17956.00000        300.00000   \n",
      "\n",
      "        next_pymnt_d  last_credit_pull_d  last_fico_range_high  \\\n",
      "33636    17987.00000         17956.00000             594.00000   \n",
      "427430   17987.00000         17956.00000             644.00000   \n",
      "487674   17987.00000         17956.00000             599.00000   \n",
      "492122   17987.00000         17956.00000             694.00000   \n",
      "521688   17987.00000         17956.00000             659.00000   \n",
      "\n",
      "        last_fico_range_low  collections_12_mths_ex_med  \\\n",
      "33636             590.00000                     0.00000   \n",
      "427430            640.00000                     0.00000   \n",
      "487674            595.00000                     0.00000   \n",
      "492122            690.00000                     0.00000   \n",
      "521688            655.00000                     0.00000   \n",
      "\n",
      "        mths_since_last_major_derog application_type  acc_now_delinq  \\\n",
      "33636                      44.16422       Individual         0.00000   \n",
      "427430                     44.16422       Individual         0.00000   \n",
      "487674                     44.16422       Individual         0.00000   \n",
      "492122                     44.16422        Joint App         0.00000   \n",
      "521688                     64.00000       Individual         0.00000   \n",
      "\n",
      "        tot_coll_amt  tot_cur_bal  open_acc_6m  open_act_il  open_il_12m  \\\n",
      "33636        0.00000 347790.00000      0.00000      0.00000      0.00000   \n",
      "427430       0.00000 285948.00000      0.00000      1.00000      1.00000   \n",
      "487674       0.00000    923.00000      0.00000      0.00000      0.00000   \n",
      "492122       0.00000  20961.00000      1.00000      2.00000      1.00000   \n",
      "521688     494.00000  30333.00000      2.00000     12.00000      3.00000   \n",
      "\n",
      "        open_il_24m  mths_since_rcnt_il  total_bal_il   il_util  open_rv_12m  \\\n",
      "33636       0.00000             0.00000       0.00000  69.14098      0.00000   \n",
      "427430      1.00000             9.00000   50373.00000  69.14098      0.00000   \n",
      "487674      1.00000            14.00000       0.00000  69.14098      0.00000   \n",
      "492122      1.00000            10.00000   12968.00000  55.00000      3.00000   \n",
      "521688      7.00000             5.00000   30022.00000 105.00000      0.00000   \n",
      "\n",
      "        open_rv_24m  max_bal_bc  all_util  total_rev_hi_lim  inq_fi  \\\n",
      "33636       0.00000     0.00000  57.03230       20000.00000 0.00000   \n",
      "427430      0.00000 11140.00000  62.00000       31500.00000 0.00000   \n",
      "487674      1.00000   814.00000  19.00000        5000.00000 2.00000   \n",
      "492122      5.00000  5608.00000  24.00000       64700.00000 0.00000   \n",
      "521688      1.00000     0.00000 103.00000        1000.00000 0.00000   \n",
      "\n",
      "        total_cu_tl  inq_last_12m  acc_open_past_24mths  avg_cur_bal  \\\n",
      "33636       0.00000       0.00000              11.00000  17390.00000   \n",
      "427430      0.00000       2.00000               1.00000  35744.00000   \n",
      "487674      0.00000       1.00000               2.00000    308.00000   \n",
      "492122      0.00000       2.00000               6.00000   1497.00000   \n",
      "521688      5.00000       1.00000               8.00000   2333.00000   \n",
      "\n",
      "        bc_open_to_buy  bc_util  chargeoff_within_12_mths  delinq_amnt  \\\n",
      "33636          0.00000  0.00000                   0.00000      0.00000   \n",
      "427430     11741.00000 62.20000                   0.00000      0.00000   \n",
      "487674      4077.00000 18.50000                   0.00000      0.00000   \n",
      "492122     40502.00000 15.80000                   0.00000      0.00000   \n",
      "521688         0.00000  0.00000                   0.00000      0.00000   \n",
      "\n",
      "        mo_sin_old_il_acct  mo_sin_old_rev_tl_op  mo_sin_rcnt_rev_tl_op  \\\n",
      "33636            194.00000              90.00000               61.00000   \n",
      "427430           140.00000             168.00000               40.00000   \n",
      "487674            14.00000             214.00000               23.00000   \n",
      "492122           172.00000             191.00000                2.00000   \n",
      "521688            94.00000              91.00000               23.00000   \n",
      "\n",
      "        mo_sin_rcnt_tl  mort_acc  mths_since_recent_bc  \\\n",
      "33636          3.00000   7.00000              14.00000   \n",
      "427430         9.00000   1.00000              40.00000   \n",
      "487674        14.00000   2.00000              23.00000   \n",
      "492122         2.00000   0.00000               2.00000   \n",
      "521688         5.00000   0.00000              14.00000   \n",
      "\n",
      "        mths_since_recent_bc_dlq  mths_since_recent_inq  \\\n",
      "33636                   37.00000                8.00000   \n",
      "427430                  40.00000                9.00000   \n",
      "487674                  19.00000                5.00000   \n",
      "492122                  37.00000               10.00000   \n",
      "521688                  37.00000                8.00000   \n",
      "\n",
      "        mths_since_recent_revol_delinq  num_accts_ever_120_pd  num_actv_bc_tl  \\\n",
      "33636                          0.00000                0.00000         0.00000   \n",
      "427430                        40.00000                0.00000         3.00000   \n",
      "487674                        19.00000                0.00000         2.00000   \n",
      "492122                         0.00000                0.00000         3.00000   \n",
      "521688                         0.00000                1.00000         0.00000   \n",
      "\n",
      "        num_actv_rev_tl  num_bc_sats  num_bc_tl  num_il_tl  num_op_rev_tl  \\\n",
      "33636           1.00000      0.00000    1.00000   51.00000        1.00000   \n",
      "427430          3.00000      5.00000    8.00000    3.00000        6.00000   \n",
      "487674          2.00000      3.00000    8.00000    1.00000        3.00000   \n",
      "492122          6.00000      8.00000   11.00000   13.00000       12.00000   \n",
      "521688          1.00000      0.00000    0.00000   15.00000        1.00000   \n",
      "\n",
      "        num_rev_accts  num_rev_tl_bal_gt_0  num_sats  num_tl_120dpd_2m  \\\n",
      "33636         2.00000              1.00000  20.00000           0.00000   \n",
      "427430       10.00000              3.00000   8.00000           0.00000   \n",
      "487674        9.00000              2.00000   3.00000           0.00000   \n",
      "492122       15.00000              6.00000  14.00000           0.00000   \n",
      "521688        2.00000              1.00000  13.00000           0.00000   \n",
      "\n",
      "        num_tl_30dpd  num_tl_90g_dpd_24m  num_tl_op_past_12m  pct_tl_nvr_dlq  \\\n",
      "33636        0.00000             0.00000             9.00000       100.00000   \n",
      "427430       0.00000             0.00000             1.00000        92.90000   \n",
      "487674       0.00000             0.00000             0.00000        83.30000   \n",
      "492122       0.00000             0.00000             4.00000       100.00000   \n",
      "521688       0.00000             0.00000             3.00000        94.40000   \n",
      "\n",
      "        percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  \\\n",
      "33636            0.00000               0.00000    0.00000     344334.00000   \n",
      "427430          40.00000               0.00000    0.00000     325807.00000   \n",
      "487674          33.30000               1.00000    0.00000       5000.00000   \n",
      "492122          12.50000               0.00000    0.00000      88409.00000   \n",
      "521688           0.00000               0.00000    0.00000      29480.00000   \n",
      "\n",
      "        total_bal_ex_mort  total_bc_limit  total_il_high_credit_limit  \\\n",
      "33636        184570.00000         0.00000                160225.00000   \n",
      "427430        69732.00000     31100.00000                 61557.00000   \n",
      "487674          923.00000      5000.00000                     0.00000   \n",
      "492122        20961.00000     48100.00000                 23709.00000   \n",
      "521688        30333.00000         0.00000                 28480.00000   \n",
      "\n",
      "       hardship_flag disbursement_method debt_settlement_flag  TARGET  \n",
      "33636              N                Cash                    N       1  \n",
      "427430             N                Cash                    N       1  \n",
      "487674             N                Cash                    N       1  \n",
      "492122             N                Cash                    N       1  \n",
      "521688             N                Cash                    N       1  \n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400020, 222), (400020,), (100020, 222), (100020,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE uses synthetic data to increase the number of samples in the minority class. It finds the n-nearest neighbors in TARGET == 1 and draws a line between each neighbors and creates random points along the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12, sampling_strategy = 'minority')\n",
    "X_train_r, y_train_r = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800000, 222), (800000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_r.shape, y_train_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, random_state=21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C = 0.0001, random_state = 21)\n",
    "\n",
    "clf.fit(X_train_r, y_train_r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.7940611877624475\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 0.8523875\n"
     ]
    }
   ],
   "source": [
    "y_predict_test = clf.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test, y_test))\n",
    "\n",
    "y_predict_training = clf.predict(X_train_r)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_r, y_predict_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84    400000\n",
      "           1       0.81      0.91      0.86    400000\n",
      "\n",
      "    accuracy                           0.85    800000\n",
      "   macro avg       0.86      0.85      0.85    800000\n",
      "weighted avg       0.86      0.85      0.85    800000\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.89    100000\n",
      "           1       0.00      0.60      0.00        20\n",
      "\n",
      "    accuracy                           0.79    100020\n",
      "   macro avg       0.50      0.70      0.44    100020\n",
      "weighted avg       1.00      0.79      0.89    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train_r, y_predict_training))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[316834  83166]\n",
      " [ 34924 365076]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[79410 20590]\n",
      " [    8    12]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7519375 0.786325  0.544025  0.65545   0.810375 ]\n",
      "Mean cross validation test score: 0.7096225\n",
      "Mean cross validation train score: 0.9115413053437502\n",
      "Standard deviation in cv test scores: 0.09816525938436674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores_test= cross_val_score(clf,X_test,y_test,cv=5,scoring='roc_auc')\n",
    "cv_scores_train= cross_val_score(clf,X_train_r,y_train_r,cv=5,scoring='roc_auc')\n",
    "print(cv_scores_test)\n",
    "cv_scores_lr_test= cv_scores_test.mean()\n",
    "cv_scores_lr_train= cv_scores_train.mean()\n",
    "cv_scores_std_test_lr= cv_scores_test.std()\n",
    "print ('Mean cross validation test score: ' +str(cv_scores_lr_test))\n",
    "print ('Mean cross validation train score: ' +str(cv_scores_lr_train))\n",
    "print ('Standard deviation in cv test scores: ' +str(cv_scores_std_test_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall score seems to have increased significantly but precision is still off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=21)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=21)\n",
    "clf_rf.fit(X_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9998000399920016\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 1.0\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_rf = clf_rf.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_rf, y_test))\n",
    "\n",
    "y_predict_training_rf = clf_rf.predict(X_train_r)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_r, y_predict_training_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    100000\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00    100020\n",
      "   macro avg       0.50      0.50      0.50    100020\n",
      "weighted avg       1.00      1.00      1.00    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[400000      0]\n",
      " [     1 399999]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[100000      0]\n",
      " [    20      0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_rf)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_rf)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.490025   0.61818125 0.61804375 0.87186875 0.865175  ]\n",
      "Mean cross validation test score: 0.69265875\n",
      "Mean cross validation train score: 0.99999999378125\n",
      "Standard deviation in cv test scores: 0.1510316612398705\n"
     ]
    }
   ],
   "source": [
    "cv_scores_test= cross_val_score(clf_rf, X_test, y_test,cv=5,scoring='roc_auc')\n",
    "cv_scores_train= cross_val_score(clf_rf, X_train_r, y_train_r,cv=5,scoring='roc_auc')\n",
    "print(cv_scores_test)\n",
    "cv_scores_rf_test= cv_scores_test.mean()\n",
    "cv_scores_rf_train= cv_scores_train.mean()\n",
    "cv_scores_std_test_rf= cv_scores_test.std()\n",
    "print ('Mean cross validation test score: ' + str(cv_scores_rf_test))\n",
    "print ('Mean cross validation train score: ' + str(cv_scores_rf_train))\n",
    "print ('Standard deviation in cv test scores: ' + str(cv_scores_std_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:35:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb = XGBClassifier()\n",
    "clf_xgb.fit(X_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9998000399920016\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 1.0\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_xgb = clf_xgb.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_xgb, y_test))\n",
    "\n",
    "y_predict_training_xgb = clf_xgb.predict(X_train_r)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_r, y_predict_training_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    400000\n",
      "           1       1.00      1.00      1.00    400000\n",
      "\n",
      "    accuracy                           1.00    800000\n",
      "   macro avg       1.00      1.00      1.00    800000\n",
      "weighted avg       1.00      1.00      1.00    800000\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    100000\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00    100020\n",
      "   macro avg       0.50      0.50      0.50    100020\n",
      "weighted avg       1.00      1.00      1.00    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train_r, y_predict_training_xgb))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[400000      0]\n",
      " [     0 400000]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[100000      0]\n",
      " [    20      0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_xgb)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_xgb)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:29:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:29:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:29:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:29:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:30:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:30:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:31:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:33:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:36:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9743    0.9164875 0.92505   0.9577875 0.8477125]\n",
      "Mean cross validation test score: 0.9242675\n",
      "Mean cross validation train score: 0.9999997887500001\n",
      "Standard deviation in cv test scores: 0.043700458092564634\n"
     ]
    }
   ],
   "source": [
    "cv_scores_test= cross_val_score(clf_xgb, X_test, y_test,cv=5,scoring='roc_auc')\n",
    "cv_scores_train= cross_val_score(clf_xgb, X_train_r, y_train_r,cv=5,scoring='roc_auc')\n",
    "print(cv_scores_test)\n",
    "cv_scores_xgb_test= cv_scores_test.mean()\n",
    "cv_scores_xgb_train= cv_scores_train.mean()\n",
    "cv_scores_std_test_xgb= cv_scores_test.std()\n",
    "print ('Mean cross validation test score: ' + str(cv_scores_xgb_test))\n",
    "print ('Mean cross validation train score: ' + str(cv_scores_xgb_train))\n",
    "print ('Standard deviation in cv test scores: ' + str(cv_scores_std_test_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lgbm = LGBMClassifier()\n",
    "clf_lgbm.fit(X_train_r, y_train_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9997900419916017\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 1.0\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_lgbm = clf_lgbm.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_lgbm, y_test))\n",
    "\n",
    "y_predict_training_lgbm = clf_lgbm.predict(X_train_r)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_r, y_predict_training_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    400000\n",
      "           1       1.00      1.00      1.00    400000\n",
      "\n",
      "    accuracy                           1.00    800000\n",
      "   macro avg       1.00      1.00      1.00    800000\n",
      "weighted avg       1.00      1.00      1.00    800000\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    100000\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00    100020\n",
      "   macro avg       0.50      0.50      0.50    100020\n",
      "weighted avg       1.00      1.00      1.00    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train_r, y_predict_training_lgbm))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[400000      0]\n",
      " [     0 400000]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[99999     1]\n",
      " [   20     0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_lgbm)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_lgbm)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.450525   0.44323125 0.2506     0.6044625  0.4995    ]\n",
      "Mean cross validation test score: 0.4496637499999999\n",
      "Mean cross validation train score: 0.999998042890625\n",
      "Standard deviation in cv test scores: 0.11499113025577236\n"
     ]
    }
   ],
   "source": [
    "cv_scores_test= cross_val_score(clf_lgbm, X_test, y_test,cv=5,scoring='roc_auc')\n",
    "cv_scores_train= cross_val_score(clf_lgbm, X_train_r, y_train_r,cv=5,scoring='roc_auc')\n",
    "print(cv_scores_test)\n",
    "cv_scores_lgbm_test= cv_scores_test.mean()\n",
    "cv_scores_lgbm_train= cv_scores_train.mean()\n",
    "cv_scores_std_test_lgbm= cv_scores_test.std()\n",
    "print ('Mean cross validation test score: ' + str(cv_scores_lgbm_test))\n",
    "print ('Mean cross validation train score: ' + str(cv_scores_lgbm_train))\n",
    "print ('Standard deviation in cv test scores: ' + str(cv_scores_std_test_lgbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling with ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADASYN is similar to SMOTE but adds additional variance i.e. generates synthetic points but not directly along the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm = ADASYN(random_state=13, sampling_strategy = 'minority')\n",
    "X_train_rr, y_train_rr = rsm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800000, 222), (800000,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rr.shape, y_train_rr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, random_state=22)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = LogisticRegression(C = 0.0001, random_state = 22)\n",
    "\n",
    "clf2.fit(X_train_rr, y_train_rr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test_alr, y_test): 0.7934113177364527\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_rr, y_predict_training_alr) 0.85052625\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_alr = clf2.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test_alr, y_test):\",accuracy_score(y_predict_test_alr, y_test))\n",
    "\n",
    "y_predict_training_alr = clf2.predict(X_train_rr)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_rr, y_predict_training_alr)\",accuracy_score(y_train_rr, y_predict_training_alr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84    400000\n",
      "           1       0.81      0.91      0.86    400000\n",
      "\n",
      "    accuracy                           0.85    800000\n",
      "   macro avg       0.86      0.85      0.85    800000\n",
      "weighted avg       0.86      0.85      0.85    800000\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88    100000\n",
      "           1       0.00      0.60      0.00        20\n",
      "\n",
      "    accuracy                           0.79    100020\n",
      "   macro avg       0.50      0.70      0.44    100020\n",
      "weighted avg       1.00      0.79      0.88    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train_rr, y_predict_training_alr))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_alr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[316579  83421]\n",
      " [ 36158 363842]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[79345 20655]\n",
      " [    8    12]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_alr)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_alr)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=40, random_state=22)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf2 = RandomForestClassifier(n_estimators=40, random_state=22)\n",
    "clf_rf2.fit(X_train_rr, y_train_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9998000399920016\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 0.99999875\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_rf2 = clf_rf2.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_rf2, y_test))\n",
    "\n",
    "y_predict_training_rf2 = clf_rf2.predict(X_train_rr)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_rr, y_predict_training_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    100000\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00    100020\n",
      "   macro avg       0.50      0.50      0.50    100020\n",
      "weighted avg       1.00      1.00      1.00    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[400000      0]\n",
      " [     1 399999]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[100000      0]\n",
      " [    20      0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_rf2)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_rf2)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:04:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb2 = XGBClassifier()\n",
    "clf_xgb2.fit(X_train_rr, y_train_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9998000399920016\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 1.0\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_xgb2 = clf_xgb2.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_xgb2, y_test))\n",
    "\n",
    "y_predict_training_xgb2 = clf_xgb2.predict(X_train_rr)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_rr, y_predict_training_xgb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    400000\n",
      "           1       1.00      1.00      1.00    400000\n",
      "\n",
      "    accuracy                           1.00    800000\n",
      "   macro avg       1.00      1.00      1.00    800000\n",
      "weighted avg       1.00      1.00      1.00    800000\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    100000\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00    100020\n",
      "   macro avg       0.50      0.50      0.50    100020\n",
      "weighted avg       1.00      1.00      1.00    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train_rr, y_predict_training_xgb2))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_xgb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[400000      0]\n",
      " [     0 400000]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[100000      0]\n",
      " [    20      0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_xgb2)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_xgb2)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lgbm2 = LGBMClassifier()\n",
    "clf_lgbm2.fit(X_train_rr, y_train_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9997900419916017\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 1.0\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_lgbm2 = clf_lgbm.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_lgbm2, y_test))\n",
    "\n",
    "y_predict_training_lgbm2 = clf_lgbm.predict(X_train_rr)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_rr, y_predict_training_lgbm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    400000\n",
      "           1       1.00      1.00      1.00    400000\n",
      "\n",
      "    accuracy                           1.00    800000\n",
      "   macro avg       1.00      1.00      1.00    800000\n",
      "weighted avg       1.00      1.00      1.00    800000\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    100000\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           1.00    100020\n",
      "   macro avg       0.50      0.50      0.50    100020\n",
      "weighted avg       1.00      1.00      1.00    100020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train_rr, y_predict_training_lgbm2))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_predict_test_lgbm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[400000      0]\n",
      " [     0 400000]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[99999     1]\n",
      " [   20     0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_lgbm2)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_lgbm2)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Model Evaulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Models Built (Resampling Technique + Classification Algorithm)\n",
    "\n",
    "> Performance Metrics for minority class from the classification\n",
    "report (including \"support\")\n",
    "\n",
    "> Performance Metrics for majority class from the classification\n",
    "report (Precision, Recall, F-1, Support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "smotemetrics = pd.read_csv('C:/Users/user/Downloads/Capstone 2 Performance Metrics SMOTE - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_train_0</td>\n",
       "      <td>0.98790</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.98000</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_train_1</td>\n",
       "      <td>0.98790</td>\n",
       "      <td>0.98000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lr_test_0</td>\n",
       "      <td>0.97520</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.98000</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lr_test_1</td>\n",
       "      <td>0.97520</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_test_0</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_test_1</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgb_train_0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xgb_train_1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgb_test_0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgb_test_1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lgbm_train_0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lgbm_train_1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm_test_0</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm_test_1</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall      F1  Support\n",
       "0     lr_train_0   0.98790    1.00000 0.98000 0.99000   400000\n",
       "1     lr_train_1   0.98790    0.98000 1.00000 0.99000   400000\n",
       "2      lr_test_0   0.97520    1.00000 0.98000 0.99000   100000\n",
       "3      lr_test_1   0.97520    0.00000 0.62000 0.00000        8\n",
       "4      rf_test_0   0.99990    1.00000 1.00000 1.00000   100000\n",
       "5      rf_test_1   0.99990    0.00000 0.00000 0.00000        8\n",
       "6    xgb_train_0   1.00000    1.00000 1.00000 1.00000   400000\n",
       "7    xgb_train_1   1.00000    1.00000 1.00000 1.00000   400000\n",
       "8     xgb_test_0   1.00000    1.00000 1.00000 1.00000   100000\n",
       "9     xgb_test_1   0.00000    0.00000 0.00000 0.00000        8\n",
       "10  lgbm_train_0   1.00000    1.00000 1.00000 1.00000   400000\n",
       "11  lgbm_train_1   1.00000    1.00000 1.00000 1.00000   400000\n",
       "12   lgbm_test_0   0.99990    1.00000 1.00000 1.00000   100000\n",
       "13   lgbm_test_1   0.99990    0.00000 0.00000 0.00000        8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(smotemetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Algorithm  Model accuracy score\n",
      "0  Logistic Regression               0.79406\n",
      "1        Random Forest               0.99980\n",
      "2                  XGB               0.99980\n",
      "3                 LGBM               0.99979\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>ROC-AUC train score</th>\n",
       "      <th>ROC-AUC test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.91154</td>\n",
       "      <td>0.70962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.69266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.92427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.44966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Algorithm  ROC-AUC train score  ROC-AUC test score\n",
       "0  Logistic Regression              0.91154             0.70962\n",
       "1        Random Forest              1.00000             0.69266\n",
       "2                  XGB              1.00000             0.92427\n",
       "3                 LGBM              1.00000             0.44966"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLabels = ['Logistic Regression', 'Random Forest','XGB', 'LGBM']\n",
    "score_test= [  cv_scores_lr_test,cv_scores_rf_test,cv_scores_xgb_test,cv_scores_lgbm_test]\n",
    "score_train= [  cv_scores_lr_train,cv_scores_rf_train,cv_scores_xgb_train,cv_scores_lgbm_train]\n",
    "Accuracy_score = [accuracy_score(y_predict_test, y_test),accuracy_score(y_predict_test_rf, y_test),accuracy_score(y_predict_test_xgb, y_test),accuracy_score(y_predict_test_lgbm, y_test)]\n",
    "\n",
    "score_tab_acc = pd.DataFrame(list(zip(myLabels, Accuracy_score)), \n",
    "               columns =['Algorithm', 'Model accuracy score']) \n",
    "\n",
    "score_tab = pd.DataFrame(list(zip(myLabels, score_train, score_test)), \n",
    "               columns =['Algorithm', 'ROC-AUC train score', 'ROC-AUC test score' ]) \n",
    "print(score_tab_acc)\n",
    "\n",
    "score_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADASYN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasynmetrics = pd.read_csv('C:/Users/user/Downloads/Capstone 2 Performance Metrics SMOTE_ADASYN - Sheet2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_train_0</td>\n",
       "      <td>0.98790</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.98000</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_train_1</td>\n",
       "      <td>0.98790</td>\n",
       "      <td>0.98000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lr_test_0</td>\n",
       "      <td>0.97520</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.98000</td>\n",
       "      <td>0.99000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lr_test_1</td>\n",
       "      <td>0.97520</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.62000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_test_0</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_test_1</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xgb_train_0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xgb_train_1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgb_test_0</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgb_test_1</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lgbm_train_0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lgbm_train_1</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lgbm_test_0</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lgbm_test_1</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall      F1  Support\n",
       "0     lr_train_0   0.98790    1.00000 0.98000 0.99000   400000\n",
       "1     lr_train_1   0.98790    0.98000 1.00000 0.99000   400000\n",
       "2      lr_test_0   0.97520    1.00000 0.98000 0.99000   100000\n",
       "3      lr_test_1   0.97520    0.00000 0.62000 0.00000        8\n",
       "4      rf_test_0   0.99990    1.00000 1.00000 1.00000   100000\n",
       "5      rf_test_1   0.99990    0.00000 0.00000 0.00000        8\n",
       "6    xgb_train_0   1.00000    1.00000 1.00000 1.00000   400000\n",
       "7    xgb_train_1   1.00000    1.00000 1.00000 1.00000   400000\n",
       "8     xgb_test_0   0.99990    1.00000 1.00000 1.00000   100000\n",
       "9     xgb_test_1   0.99990    0.00000 0.00000 0.00000        8\n",
       "10  lgbm_train_0   1.00000    1.00000 1.00000 1.00000   400000\n",
       "11  lgbm_train_1   1.00000    1.00000 1.00000 1.00000   400000\n",
       "12   lgbm_test_0   0.99990    1.00000 1.00000 1.00000   100000\n",
       "13   lgbm_test_1   0.99990    0.00000 0.00000 0.00000        8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(adasynmetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch CV with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 100.0, 'penalty': 'l2'}\n",
      "accuracy : 0.8526724999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train_r,y_train_r)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.7917816436712658\n"
     ]
    }
   ],
   "source": [
    "logreg2=LogisticRegression(C=1,penalty=\"l2\")\n",
    "logreg2.fit(X_train_r,y_train_r)\n",
    "print(\"score\",logreg2.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.7917816436712658\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 0.859065\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_logreg2 = logreg2.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_logreg2, y_test))\n",
    "\n",
    "y_predict_training_logreg2 = logreg2.predict(X_train_r)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_r, y_predict_training_logreg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[315962  84038]\n",
      " [ 28710 371290]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[79185 20815]\n",
      " [   11     9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_logreg2)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_logreg2)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -0.00025\n",
      "Feature: 1, Score: -0.00024\n",
      "Feature: 2, Score: -0.00022\n",
      "Feature: 3, Score: -0.00001\n",
      "Feature: 4, Score: 0.00000\n",
      "Feature: 5, Score: 0.00005\n",
      "Feature: 6, Score: 0.00000\n",
      "Feature: 7, Score: -0.00002\n",
      "Feature: 8, Score: 0.00005\n",
      "Feature: 9, Score: -0.00000\n",
      "Feature: 10, Score: -0.00000\n",
      "Feature: 11, Score: 0.00016\n",
      "Feature: 12, Score: 0.00000\n",
      "Feature: 13, Score: 0.00000\n",
      "Feature: 14, Score: -0.00000\n",
      "Feature: 15, Score: 0.00001\n",
      "Feature: 16, Score: -0.00001\n",
      "Feature: 17, Score: 0.00000\n",
      "Feature: 18, Score: 0.00000\n",
      "Feature: 19, Score: -0.00024\n",
      "Feature: 20, Score: -0.00001\n",
      "Feature: 21, Score: 0.00000\n",
      "Feature: 22, Score: 0.00033\n",
      "Feature: 23, Score: 0.00033\n",
      "Feature: 24, Score: 0.00021\n",
      "Feature: 25, Score: 0.00023\n",
      "Feature: 26, Score: -0.00009\n",
      "Feature: 27, Score: 0.00031\n",
      "Feature: 28, Score: 0.00005\n",
      "Feature: 29, Score: -0.00006\n",
      "Feature: 30, Score: -0.00001\n",
      "Feature: 31, Score: 0.00015\n",
      "Feature: 32, Score: -0.00038\n",
      "Feature: 33, Score: -0.00016\n",
      "Feature: 34, Score: -0.00000\n",
      "Feature: 35, Score: -0.00019\n",
      "Feature: 36, Score: -0.00018\n",
      "Feature: 37, Score: 0.00000\n",
      "Feature: 38, Score: 0.00000\n",
      "Feature: 39, Score: -0.00000\n",
      "Feature: 40, Score: -0.00009\n",
      "Feature: 41, Score: 0.00005\n",
      "Feature: 42, Score: -0.00000\n",
      "Feature: 43, Score: 0.00000\n",
      "Feature: 44, Score: 0.00000\n",
      "Feature: 45, Score: 0.00000\n",
      "Feature: 46, Score: 0.00001\n",
      "Feature: 47, Score: 0.00000\n",
      "Feature: 48, Score: 0.00000\n",
      "Feature: 49, Score: 0.00000\n",
      "Feature: 50, Score: 0.00000\n",
      "Feature: 51, Score: 0.00016\n",
      "Feature: 52, Score: -0.00000\n",
      "Feature: 53, Score: 0.00005\n",
      "Feature: 54, Score: -0.00000\n",
      "Feature: 55, Score: 0.00000\n",
      "Feature: 56, Score: 0.00000\n",
      "Feature: 57, Score: 0.00000\n",
      "Feature: 58, Score: 0.00001\n",
      "Feature: 59, Score: -0.00020\n",
      "Feature: 60, Score: -0.00001\n",
      "Feature: 61, Score: -0.00000\n",
      "Feature: 62, Score: -0.00001\n",
      "Feature: 63, Score: 0.00001\n",
      "Feature: 64, Score: -0.00002\n",
      "Feature: 65, Score: 0.00001\n",
      "Feature: 66, Score: -0.00000\n",
      "Feature: 67, Score: -0.00000\n",
      "Feature: 68, Score: 0.00001\n",
      "Feature: 69, Score: -0.00000\n",
      "Feature: 70, Score: 0.00000\n",
      "Feature: 71, Score: -0.00000\n",
      "Feature: 72, Score: -0.00000\n",
      "Feature: 73, Score: 0.00000\n",
      "Feature: 74, Score: -0.00000\n",
      "Feature: 75, Score: 0.00000\n",
      "Feature: 76, Score: -0.00000\n",
      "Feature: 77, Score: 0.00000\n",
      "Feature: 78, Score: 0.00000\n",
      "Feature: 79, Score: 0.00000\n",
      "Feature: 80, Score: -0.00000\n",
      "Feature: 81, Score: 0.00000\n",
      "Feature: 82, Score: -0.00000\n",
      "Feature: 83, Score: -0.00000\n",
      "Feature: 84, Score: -0.00000\n",
      "Feature: 85, Score: 0.00000\n",
      "Feature: 86, Score: 0.00001\n",
      "Feature: 87, Score: -0.00001\n",
      "Feature: 88, Score: -0.00000\n",
      "Feature: 89, Score: 0.00000\n",
      "Feature: 90, Score: -0.00004\n",
      "Feature: 91, Score: -0.00007\n",
      "Feature: 92, Score: 0.00019\n",
      "Feature: 93, Score: 0.00005\n",
      "Feature: 94, Score: -0.00000\n",
      "Feature: 95, Score: -0.00000\n",
      "Feature: 96, Score: -0.00000\n",
      "Feature: 97, Score: -0.00000\n",
      "Feature: 98, Score: -0.00000\n",
      "Feature: 99, Score: -0.00000\n",
      "Feature: 100, Score: -0.00000\n",
      "Feature: 101, Score: -0.00000\n",
      "Feature: 102, Score: -0.00000\n",
      "Feature: 103, Score: -0.00000\n",
      "Feature: 104, Score: -0.00000\n",
      "Feature: 105, Score: -0.00000\n",
      "Feature: 106, Score: -0.00000\n",
      "Feature: 107, Score: -0.00000\n",
      "Feature: 108, Score: -0.00000\n",
      "Feature: 109, Score: -0.00000\n",
      "Feature: 110, Score: -0.00000\n",
      "Feature: 111, Score: -0.00000\n",
      "Feature: 112, Score: -0.00000\n",
      "Feature: 113, Score: -0.00000\n",
      "Feature: 114, Score: -0.00000\n",
      "Feature: 115, Score: -0.00000\n",
      "Feature: 116, Score: -0.00000\n",
      "Feature: 117, Score: -0.00000\n",
      "Feature: 118, Score: -0.00000\n",
      "Feature: 119, Score: -0.00000\n",
      "Feature: 120, Score: -0.00000\n",
      "Feature: 121, Score: -0.00000\n",
      "Feature: 122, Score: -0.00000\n",
      "Feature: 123, Score: -0.00000\n",
      "Feature: 124, Score: -0.00000\n",
      "Feature: 125, Score: -0.00000\n",
      "Feature: 126, Score: -0.00000\n",
      "Feature: 127, Score: -0.00000\n",
      "Feature: 128, Score: -0.00000\n",
      "Feature: 129, Score: -0.00000\n",
      "Feature: 130, Score: -0.00000\n",
      "Feature: 131, Score: -0.00000\n",
      "Feature: 132, Score: -0.00000\n",
      "Feature: 133, Score: -0.00000\n",
      "Feature: 134, Score: -0.00000\n",
      "Feature: 135, Score: -0.00000\n",
      "Feature: 136, Score: -0.00000\n",
      "Feature: 137, Score: -0.00000\n",
      "Feature: 138, Score: -0.00000\n",
      "Feature: 139, Score: -0.00000\n",
      "Feature: 140, Score: -0.00000\n",
      "Feature: 141, Score: -0.00000\n",
      "Feature: 142, Score: -0.00000\n",
      "Feature: 143, Score: -0.00000\n",
      "Feature: 144, Score: -0.00000\n",
      "Feature: 145, Score: -0.00000\n",
      "Feature: 146, Score: -0.00000\n",
      "Feature: 147, Score: -0.00000\n",
      "Feature: 148, Score: -0.00000\n",
      "Feature: 149, Score: -0.00000\n",
      "Feature: 150, Score: -0.00000\n",
      "Feature: 151, Score: -0.00000\n",
      "Feature: 152, Score: -0.00000\n",
      "Feature: 153, Score: -0.00000\n",
      "Feature: 154, Score: -0.00000\n",
      "Feature: 155, Score: -0.00000\n",
      "Feature: 156, Score: -0.00000\n",
      "Feature: 157, Score: -0.00000\n",
      "Feature: 158, Score: -0.00000\n",
      "Feature: 159, Score: -0.00000\n",
      "Feature: 160, Score: -0.00000\n",
      "Feature: 161, Score: -0.00000\n",
      "Feature: 162, Score: -0.00000\n",
      "Feature: 163, Score: -0.00000\n",
      "Feature: 164, Score: -0.00000\n",
      "Feature: 165, Score: -0.00000\n",
      "Feature: 166, Score: -0.00000\n",
      "Feature: 167, Score: -0.00000\n",
      "Feature: 168, Score: -0.00000\n",
      "Feature: 169, Score: -0.00000\n",
      "Feature: 170, Score: -0.00000\n",
      "Feature: 171, Score: -0.00000\n",
      "Feature: 172, Score: -0.00000\n",
      "Feature: 173, Score: -0.00000\n",
      "Feature: 174, Score: -0.00000\n",
      "Feature: 175, Score: 0.00000\n",
      "Feature: 176, Score: -0.00000\n",
      "Feature: 177, Score: -0.00000\n",
      "Feature: 178, Score: -0.00000\n",
      "Feature: 179, Score: -0.00000\n",
      "Feature: 180, Score: -0.00000\n",
      "Feature: 181, Score: -0.00000\n",
      "Feature: 182, Score: -0.00000\n",
      "Feature: 183, Score: -0.00000\n",
      "Feature: 184, Score: -0.00000\n",
      "Feature: 185, Score: -0.00000\n",
      "Feature: 186, Score: -0.00000\n",
      "Feature: 187, Score: -0.00000\n",
      "Feature: 188, Score: -0.00000\n",
      "Feature: 189, Score: -0.00000\n",
      "Feature: 190, Score: -0.00000\n",
      "Feature: 191, Score: -0.00000\n",
      "Feature: 192, Score: -0.00000\n",
      "Feature: 193, Score: -0.00000\n",
      "Feature: 194, Score: -0.00000\n",
      "Feature: 195, Score: -0.00000\n",
      "Feature: 196, Score: -0.00000\n",
      "Feature: 197, Score: -0.00000\n",
      "Feature: 198, Score: -0.00000\n",
      "Feature: 199, Score: -0.00000\n",
      "Feature: 200, Score: -0.00000\n",
      "Feature: 201, Score: -0.00000\n",
      "Feature: 202, Score: -0.00000\n",
      "Feature: 203, Score: -0.00000\n",
      "Feature: 204, Score: -0.00000\n",
      "Feature: 205, Score: -0.00000\n",
      "Feature: 206, Score: -0.00000\n",
      "Feature: 207, Score: -0.00000\n",
      "Feature: 208, Score: -0.00000\n",
      "Feature: 209, Score: -0.00000\n",
      "Feature: 210, Score: -0.00000\n",
      "Feature: 211, Score: -0.00000\n",
      "Feature: 212, Score: -0.00000\n",
      "Feature: 213, Score: 0.00000\n",
      "Feature: 214, Score: -0.00000\n",
      "Feature: 215, Score: -0.00000\n",
      "Feature: 216, Score: -0.00000\n",
      "Feature: 217, Score: -0.00000\n",
      "Feature: 218, Score: 0.00000\n",
      "Feature: 219, Score: -0.00000\n",
      "Feature: 220, Score: -0.00000\n",
      "Feature: 221, Score: 0.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = logreg2.coef_[0]\n",
    "# summarizing feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [(X_test,y_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.985761\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.983958\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.984689\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.985715\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.986218\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.987228\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.992246\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's auc: 0.991087\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.990789\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.990615\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.99161\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.991267\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.987611\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.98145\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.987994\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.940245\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid's auc: 0.983633\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.93959\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.919658\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.976529\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.976145\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.978218\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's auc: 0.980255\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.933567\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.934143\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.964411\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.923699\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.92215\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.910533\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.974294\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.933373\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.976099\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.977687\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.983643\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.980228\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.974432\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.976414\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.972252\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.975595\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.985281\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's auc: 0.985572\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.973781\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.977584\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.991423\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.991266\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's auc: 0.99154\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.990742\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.990437\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.987581\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.989306\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.979716\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.982939\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.985615\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.983771\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.985773\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.98446\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.976553\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.979779\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.981689\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.979582\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.979263\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.986124\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.983528\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.98197\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.923207\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.960947\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.955824\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.985799\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.985759\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.975996\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.968014\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.970274\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.972723\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.987495\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.981492\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.982389\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.96647\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.972291\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.977897\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's auc: 0.982888\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.981856\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.982458\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's auc: 0.991672\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.992251\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's auc: 0.991594\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.97347\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.977149\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.982451\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid's auc: 0.983572\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.981366\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.98402\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.984819\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.987624\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.981815\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.986028\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.938553\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.93716\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid's auc: 0.97499\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.983638\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.985648\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.98375\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.986193\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.988998\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.989035\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.98893\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.98936\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.980272\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.983881\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.978753\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.978939\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.918146\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.977662\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.991326\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.991195\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.991546\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.972619\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.980144\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.976562\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.982664\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.983576\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.985875\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.978503\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.97846\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.976703\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.983704\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.976275\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.981508\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.991931\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's auc: 0.991382\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.992459\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.986945\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.986637\n",
      "Early stopping, best iteration is:\n",
      "[74]\tvalid's auc: 0.98984\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.986939\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.977922\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.981604\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.983824\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.985492\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.97868\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.983096\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.973382\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.982734\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.981823\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.993451\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid's auc: 0.993054\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.99254\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.976751\n",
      "Early stopping, best iteration is:\n",
      "[74]\tvalid's auc: 0.982148\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.980838\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.983193\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.95446\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.938903\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.976311\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.960534\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.937593\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.980965\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.986637\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.98795\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid's auc: 0.988972\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.991576\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.992631\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.992888\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.976996\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.979042\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.978643\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.98024\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.974567\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.976498\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.991973\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.991563\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.991503\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.991572\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.991711\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.99168\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.980388\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.980572\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.976278\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.977763\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.98007\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.971785\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.959398\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.985192\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.985352\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.984049\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.991874\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.991166\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.991688\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.991749\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.990491\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.992605\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.987101\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.979787\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.985535\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.978264\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.984541\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's auc: 0.970723\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.969604\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.974895\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.909716\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.910209\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.930327\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.987858\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.987105\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.988182\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.957802\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.955858\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.955608\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid's auc: 0.982874\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.98506\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.987545\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.98582\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid's auc: 0.976676\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.955931\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.956054\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.961899\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.984327\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.987102\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.989822\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.966351\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.975539\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.977742\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.982038\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.979591\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.981451\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.983831\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.988286\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.984349\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.978221\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.965309\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.942864\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.958293\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.982992\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.983223\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.983698\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.985788\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.9688\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.973978\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.968098\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.974776\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's auc: 0.983946\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.980663\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.972476\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's auc: 0.973746\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.986498\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.988347\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.985977\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.986811\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.988584\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.991988\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's auc: 0.991315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.992\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.991633\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.99176\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.988267\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.984824\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.982611\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.98323\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.978535\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.977657\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.978633\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.992048\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's auc: 0.990452\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.991394\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.986091\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.985931\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.985511\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.985573\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.980788\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.980901\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's auc: 0.98574\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.982664\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.974214\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's auc: 0.97934\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.981001\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.982353\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.984513\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.981472\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's auc: 0.987818\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.987712\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.988962\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.990684\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.957638\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.95697\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.970327\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.975541\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.981218\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.953714\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.984371\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.983085\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.985314\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.960611\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.984356\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.987123\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.955812\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.951943\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.977966\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.934732\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.922791\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.932353\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.966923\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.983723\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.967409\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.972391\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.983595\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.974402\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.983189\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.980922\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.98326\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.980406\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.983615\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.985867\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.917456\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.94728\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.935912\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's auc: 0.923515\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.919928\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's auc: 0.931199\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.980822\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid's auc: 0.98263\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.98441\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.985862\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.9839\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.986919\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.98583\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.980889\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.980916\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.991193\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.992352\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid's auc: 0.990248\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.892119\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.929272\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.976519\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.957593\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.957199\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.95768\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.957187\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.954276\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.921964\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.94857\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.979702\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.97939\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid's auc: 0.982318\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid's auc: 0.983495\n",
      "Best score reached: 0.9999999683500791 with params: {'colsample_bytree': 0.446235782340401, 'min_child_samples': 313, 'min_child_weight': 10.0, 'num_leaves': 31, 'reg_alpha': 2, 'reg_lambda': 10, 'subsample': 0.9969185790275723} \n"
     ]
    }
   ],
   "source": [
    "gs.fit(X_train_r, y_train_r, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_parameters = {'colsample_bytree': 0.9234, 'min_child_samples': 399, 'min_child_weight': 0.1, 'num_leaves': 13, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.855}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.9234, metric='None', min_child_samples=399,\n",
       "               min_child_weight=0.1, n_estimators=5000, n_jobs=4, num_leaves=13,\n",
       "               random_state=314, reg_alpha=2, reg_lambda=5, subsample=0.855)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_sw = lgb.LGBMClassifier(**clf.get_params())\n",
    "\n",
    "clf_sw.set_params(**opt_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_sample_weight = GridSearchCV(estimator=clf_sw, \n",
    "                                param_grid={'scale_pos_weight':[1,2,6,12]},\n",
    "                                scoring='roc_auc',\n",
    "                                cv=5,\n",
    "                                refit=True,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.966499\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.977294\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.966332\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.962666\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.957689\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.950429\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.94931\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.95395\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.960129\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.959343\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.981981\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.985344\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.984811\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.984044\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.983006\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.975039\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.968189\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.972814\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.979424\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.977285\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.92622\n",
      "Best score reached: 0.9999601319375 with params: {'scale_pos_weight': 12} \n"
     ]
    }
   ],
   "source": [
    "gs_sample_weight.fit(X_train_r, y_train_r, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs_sample_weight.best_score_, gs_sample_weight.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.979931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.9234, metric='None', min_child_samples=399,\n",
       "               min_child_weight=0.1, n_estimators=5000, n_jobs=4, num_leaves=13,\n",
       "               random_state=314, reg_alpha=2, reg_lambda=5, subsample=0.855)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_final = lgb.LGBMClassifier(**clf.get_params())\n",
    "\n",
    "clf_final.set_params(**opt_parameters)\n",
    "\n",
    "clf_final.fit(X_train_r, y_train_r, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score (y_predict_test, y_test): 0.9909518096380724\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train_r, y_predict_training) 0.99556625\n"
     ]
    }
   ],
   "source": [
    "y_predict_test_final = clf_final.predict(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score (y_predict_test, y_test):\",accuracy_score(y_predict_test_final, y_test))\n",
    "\n",
    "y_predict_training_final = clf_final.predict(X_train_r)\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train_r, y_predict_training)\",accuracy_score(y_train_r, y_predict_training_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Train: \n",
      " [[396461   3539]\n",
      " [     8 399992]]\n",
      "\n",
      "Confusion Matrix Test: \n",
      " [[99113   887]\n",
      " [   18     2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix Train: \\n {}\\n\".format(confusion_matrix(y_train_r, y_predict_training_final)))   \n",
    "print(\"Confusion Matrix Test: \\n {}\\n\".format(confusion_matrix(y_test, y_predict_test_final)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_imp = pd.Series(clf_final.feature_importances_, index=X_train_r.columns)\n",
    "feat_imp.nlargest(20).plot(kind='barh', figsize=(8,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probabilities = clf_final.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59438926, 0.40561074],\n",
       "       [0.59427656, 0.40572344],\n",
       "       [0.59434642, 0.40565358],\n",
       "       ...,\n",
       "       [0.50484076, 0.49515924],\n",
       "       [0.50452621, 0.49547379],\n",
       "       [0.59417964, 0.40582036]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this classification problem we have used 4 Classification algorithms:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGB\n",
    "- LGBM\n",
    "\n",
    "combined with with 2 resampling methods:\n",
    "\n",
    "- SMOTE\n",
    "- ADASYN\n",
    "\n",
    "This was a heavily imbalanced dataset and we ran into some major overfitting problems from the get go. Most of the models performed very poorly and initially, the primary goal was to get the model to predict at least 1 outcome. I ended up using a combination of undersampling and oversampling where I took a sample of 500 thousand and evaulated each model based on the confusion matrix and accuracy. \n",
    "\n",
    "Next we hyperparameter tuned LGBM and Logistic Regression with grid search CV. Viewing the confusion matrix of LGBM in the context of a buisness, the outcomes were significantly better with only 887 False Negatives compared to the orignal 20590 from Logistic Regression.\n",
    "\n",
    "The final outcomes are stored in the array **probabilities** where annual income, grade, remaining outstanding principle, months since recent balance, and application type had the highest feature importance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
